\documentclass[leqno]{report}
\input{base.tex}
\begin{document}
\author{Chris Wong}
\title{Properties of fast converging Cauchy sequences}
\maketitle

When computing with real numbers, we often use approximate representations such as IEEE-754 floating point. These approximations are efficient and widely supported, and give useful results most of the time.

However, these methods are not without their flaws. For use cases such as computer-assisted proofs, even the slightest chance of error is not acceptable. And even in cases which allow imprecision, is possible for a handful of floating-point operations to yield a completely incorrect result. It is these issues that motivate the study of exact representations of computable real numbers.

\chapter{Approaches to exact reals}

There are many ways to represent real numbers in a computer program. Some of these methods are outlined below.

\section{Decimal expansion}

One naive approach is to represent a real number by a sequence of digits in some base $b$. This base can be any integer greater than 1, but is usually set to 2 or 10. In the latter case, this is our usual \textit{decimal expansion}.

Formally, a non-negative real number $r = a_0 . a_1 a_2 a_3 \ldots$ is defined as the sum
\[
    r = \sum_{k=0}^\infty \frac{a_k}{b^{-k}},
\]
where $a_0$ forms the integer part, and $0 \leq a_1, a_2, \ldots < b$ form the fractional part. If the expansion ends in zeroes, then it is said to be \textit{finite}.

While this representation is easy to understand, it has drawbacks that make it hard to use in practice. First, it is not unique: the decimal expansions $0.999\ldots$ and $1.000\ldots$ are two different representations of 1. We can preserve uniqueness by removing sequences that end in nines, but that leads to issues elsewhere. For example, given the expression $0.\overline{09} + 0.\overline{90}$, it would take an infinite number of steps to decide whether the first digit is $0$ or $1$.

This problem touches on a fundamental weakness of the decimal representation. Since computing more digits can only \emph{increase} the approximation, not decrease it, each term then forms a lower bound to the true value. In other words, if we know that the number $x$ starts with the digit $1$, then we also know that $x \geq 1$. But as the addition example shows, it is undecidable in general whether or not a real number satisfies such a bound. For this reason, decimal expansions are not suited to constructive analysis.

\section{Cauchy sequences}

An alternative formulation is \textit{Cauchy sequences}, a sequence of rational numbers which become ``arbitrarily close'' to each other as the sequence progresses. In the classical definition, a sequence is Cauchy when for any bound $\varepsilon > 0$, there is an index $N$ after which any pair of points differ by less than $\varepsilon$.

For the purposes of computation, we also require that the sequence is \textit{fast converging}; that is, the sequence converges exponentially to its limit point. This restriction lets us find a suitable approximation in reasonable time. Other formulations require the denominator of each element to be a successive power of $2$. This restriction simplifies the implementation, at the cost of complicating the theory.

\section{Continued fractions}

A \textit{(simple) continued fraction} is a possibly-infinite sequence of integers $[a_0; a_1, a_2, \ldots, a_n]$, which represents the real number
\[
    a_0 + \cfrac{1}{a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n}}}}
    \ .
\]

This representation has many useful properties. It is unique; it is rational if and only if the sequence is finite; the sequences for $e$, $\sqrt{n}$ for any integer $n$, and the golden ratio $\phi$ are easy to compute. Compared to Cauchy sequences it has two drawbacks: the underlying theory is not as easy to understand; and in practice its operations are slower by a constant factor.

\chapter{Fast converging Cauchy sequences}

For the remainder of this thesis, we will discuss \textit{fast converging Cauchy sequences} (\FCCS). These differ from the usual definition in that they must converge exponentially to their limit point. Despite this restriction, \FCCS{} are equivalent to Cauchy sequences, and hence form a model for the (computable) real numbers.

\begin{Definition}
    A fast converging Cauchy sequence (\FCCS) $x : \N \rightarrow \Q$ has the property that
    \[ \fa{m, n \in \N}{ \abs{x_m - x_{m+n}} < \frac{1}{2^m} } \ . \]
\end{Definition}

\begin{Proposition}
    Every fast converging Cauchy sequence is a Cauchy sequence, and vice versa.
\end{Proposition}

\begin{proof}
    ($\Rightarrow$) Let $x$ be a \FCCS, $\varepsilon > 0$ arbitrary. Let $m$ be the least natural number such that $1/2^m \leq \varepsilon$. Then for all $n \in \N$, $\abs{x_m - x_{m+n}} < 1/2^m \leq \varepsilon$.

    ($\Leftarrow$) Let $x$ be a Cauchy sequence. Define $y_m := x_{N(1/2^m)}$ where $N(\varepsilon)$ is the index where all subsequent points differ by less than $\varepsilon$. Then $y$ is a \FCCS.
\end{proof}

\section{Arithmetic and closure properties}

With Cauchy sequences, we define arithmetic operations simply by working pointwise on each element. But with \FCCS, we need to do extra work to preserve the fast convergence property. Take addition, for example: when two approximations are added together, their errors are added as well. In general, this means that the error of the result is \textit{twice} that of the inputs. Hence the addition routine must shift the inputs one step to the right, halving the error to compensate.

\begin{Proposition}[Addition]
    \label{add}
    Let $x, y$ be \FCCS.

    The sum $x + y$, defined by $(x+y)_k := x_{k+1} + y_{k+1}$, is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    Then
    \begin{align*}
        \abs{(x+y)_m - (x+y)_{m+n}}
        &= \abs{(x_{m+1} + y_{m+1}) - (x_{m+n+1} + y_{m+n+1})} \\
        &= \abs{x_{m+1} - x_{m+n+1} + y_{m+1} - y_{m+n+1}} \\
        &\leq \abs{x_{m+1} - x_{m+n+1}} + \abs{y_{m+1} - y_{m+n+1}} \\
        &< \frac{1}{2^{m+1}} + \frac{1}{2^{m+1}} \\
        &= \frac{1}{2^m}
        \ . \qedhere
    \end{align*}
\end{proof}

\begin{Proposition}[Negation]
    \label{neg}
    If $x$ is a \FCCS, then $(-x)_k := -(x_k)$ is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    Then
    \[
        \abs{(-x)_m - (-x)_{m+n}}
        = \abs{-x_m - (-x_{m+n})}
        = \abs{x_m - x_{m+n}}
        < \frac{1}{2^m}
        \ . \qedhere
    \]
\end{proof}

\begin{Corollary}[Subtraction]
    $(x-y)_k := x_{k+1} - y_{k+1}$ is a \FCCS.
\end{Corollary}

Multiplication is more subtle. Unlike with addition, the shift under multiplication is not constant. A larger input would necessitate a larger shift, since the multiplication operation scales the error up by a corresponding amount.

\begin{Proposition}[Multiplication]
    $(xy)_k := x_{a(k)} \cdot y_{b(k)}$ is a \FCCS, where
    \begin{align*}
        a(k) &= k + 1 + \phi(y_0) \\
        b(k) &= k + 1 + \phi(x_0) \\
        \phi(z) &= \max \menge{ 0,\, \left\lceil \log_2(\abs{z} + 1) \right\rceil }
        \ .
    \end{align*}
\end{Proposition}

\begin{proof}
    Given $m, n \in \N$, we need to choose $a,\, b : \N \rightarrow \N$ such that
    \begin{align*}
        \abs{(xy)_m - (xy)_{m+n}}
        &= \abs{x_{a(m)} y_{b(m)} - x_{a(m+n)} y_{b(m+n)}} \\
        &= \abs{x_{a(m)} y_{b(m)} - x_{a(m)} y_{b(m+n)} + x_{a(m)} y_{b(m+n)} - x_{a(m+n)} y_{b(m+n)}} \\
        &\leq \abs{x_{a(m)} y_{b(m)} - x_{a(m)} y_{b(m+n)}} + \abs{x_{a(m)} y_{b(m+n)} - x_{a(m+n)} y_{b(m+n)}} \\
        &= \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}} + \abs{y_{b(m+n)}}\abs{x_{a(m)} - x_{a(m+n)}} \\
        &< \frac{1}{2^m}
        \ .
    \end{align*}

    For this inequality to hold, it is sufficient to show that
    \[ \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}} < \frac{1}{2^{m+1}} \]
    and
    \[ \abs{y_{b(m+n)}}\abs{x_{a(m)} - x_{a(m+n)}} < \frac{1}{2^{m+1}} \ . \]

    To complete the proof, we observe that $\abs{x_{a(m)}} < \abs{x_0} + 1$ and $\phi(x_0) \geq \log_2(\abs{x_0} + 1)$. Using these facts:
    \begin{align*}
        \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}}
        &< (\abs{x_0} + 1) \cdot \frac{1}{2^{b(m)}} \\
        &= \frac{\abs{x_0} + 1}{2^{m + 1 + \phi(x_0)}} \\
        &\leq \frac{\abs{x_0} + 1}{2^{m + 1 + \log_2(\abs{x_0} + 1)}} \\
        &= \frac{\abs{x_0} + 1}{2^{m + 1} \cdot (\abs{x_0} + 1)} \\
        &= \frac{1}{2^{m + 1}}
        \ .
    \end{align*}

    Similarly,
    \begin{align*}
        \abs{y_{a+b}}\abs{x_a - x_{a+b}}
        &< (\abs{y_0} + 1) \cdot \frac{1}{2^{m + 1 + \log_2(\abs{y_0} + 1)}} \\
        &= \frac{1}{2^{m + 1}}
        \ . \qedhere
    \end{align*}
\end{proof}

To derive the final arithmetic operation, division, we must introduce the idea of equality first.

\section{Ordering}

Constructive analysis presents unique challenges in defining a total order on the reals. Without double negation elimination, $a < b$ and $a \geq b$ are not full inverses; we are forced to favor one over the other. Many classical concepts, such as the sign function or equality (!), fall apart in a constructive setting.

\begin{Definition}[Equality]
    A pair of \FCCS{} $x$ and $y$ are equal when
    \[
        \fa{n \in \N}{\abs{x_n - y_n} \leq \frac{1}{2^n}}
    \]
\end{Definition}

\begin{Proposition} \label{equality_undecidable}
    Equality on \FCCS{} is undecidable.
\end{Proposition}

\begin{proof}
    Let $P$ be an arbitrary computer program. Define the \FCCS{} $x$ as follows:
    \begin{itemize}
        \item If $P$ terminates in $p$ steps, let $x_n := 1/2^{\min\menge{p, n}}$.
        \item Otherwise, let $x_n := 1/2^n$.
    \end{itemize}
    In other words, $x \rightarrow 1/2^p \neq 0$ if and only if $P$ terminates.

    Suppose there is an algorithm for deciding whether $x = 0$. Then we also have an algorithm that decides whether an arbitrary program terminates. Hence the Halting Problem is solved, and we have a contradiction.
\end{proof}

\begin{Corollary}
    The sign function
    \[
        \mathrm{sgn}(x) := \begin{cases}
            -1 & x < 0 \\
            0 & x = 0 \\
            1 & x > 0
        \end{cases}
    \]
    is undecidable.
\end{Corollary}

\begin{proof}
    If the sign function was computable, then we could determine whether $x$ and $y$ are equal by evaluating $\mathrm{sgn}(x - y)$. This contradicts Proposition~\ref{equality_undecidable}.
\end{proof}

\begin{Definition}[Ordering operators]
    $x < y$ when there exists $c \in \N$ such that
    \[ \fa{k \in \N}{y_{c+k} - x_{c+k} > \frac{1}{2^c}} \ . \]

    $x \leq y$ is equivalent to $\neg (y < x)$.

    $x \neq y$ is equivalent to $\neg (x = y)$.
\end{Definition}

\begin{Proposition}[Apartness from zero] \label{apartness}
    Assume that Markov's principle is true. Then if $x \neq 0$, either $x < 0$ or $x > 0$.
\end{Proposition}

\begin{proof}
    If $x \neq 0$, then by Markov's principle
    \[
        \neg \left( \fa{n \in \N}{\abs{x_n} \leq \frac{1}{2^n}} \right)
        \quad \Rightarrow \quad
        \ex{n \in \N}{\abs{x_n} > \frac{1}{2^n}} \ .
    \]

    If $\abs{x_n} > 1/2^n$, then either $x_n > 1/2^n$ or $-x_n > 1/2^n$. Assume the former case.

    Applying the definition of a \FCCS, for all $k \in \N$
    \begin{align*}
        \abs{x_n - x_{n+k}} < \frac{1}{2^n}
        \enspace&\Rightarrow\enspace x_n - x_{n+k} < \frac{1}{2^n} \\
        \enspace&\Rightarrow\enspace x_n - \frac{1}{2^n} < x_{n+k} \ .
    \end{align*}

    Let $c \geq n$ be the least natural number such that $1/2^c \leq x_n - 1/2^n$. Then
    \[ x_{c+k} > x_n - \frac{1}{2^n} \geq \frac{1}{2^c} \]

    Since $k$ is arbitrary, this shows that $x > 0$. With a similar argument, it can be shown that the other case implies $x < 0$.
\end{proof}

\begin{Corollary}[Apartness]
    If $x \neq y$, then by Markov's principle either $x < y$ or $x > y$.
\end{Corollary}

\begin{proof}
    Let $z := x - y$. TODO PROVE THAT $Z \neq 0$. Then by Proposition~\ref{apartness}, either $z < 0$ or $z > 0$. From that we conclude $x < y$ or $x > y$ respectively.
\end{proof}

The use of Markov's principle here is interesting, because it operationally represents an unbounded search. If the inputs are in fact equal, then the proof would search forever for evidence that does not exist. Since equality of real numbers is undecidable, this behavior is the best we can hope for.

\section{Division}

To derive the reciprocal, and with it division, observe that $x^{-1}$ is defined only when $x$ is not zero. Hence intuitively, any constructive evidence that a number is not zero should assist in computing its reciprocal.

\begin{Proposition}[Reciprocal]
    Let $x$ be a \FCCS{} where $x \neq 0$. Then $\left(x^{-1}\right)_k := \left(x_{k+2c}\right)^{-1}$ is a \FCCS, where $c$ is defined as in Proposition~\ref{apartness}.
\end{Proposition}

\begin{proof}
    By the definition of a \FCCS, we know that
    \[
        \abs{x_{m+2c} - x_{m+n+2c}} < \frac{1}{2^{m+2c}} \ ;
    \]

    And by Proposition~\ref{apartness}, we know that
    \begin{align*}
        \abs{x_{m+2c}} > \frac{1}{2^c} \enspace\wedge\enspace
        \abs{x_{m+n+2c}} > \frac{1}{2^c}
        \enspace&\Rightarrow\enspace
        \abs{x_{m+2c}} \abs{x_{m+n+2c}} > \frac{1}{2^{2c}} \\
        &\Rightarrow\enspace
        \frac{1}{\abs{x_{m+2c}} \abs{x_{m+n+2c}}} < 2^{2c} \ .
    \end{align*}

    Therefore
    \begin{align*}
        \abs{\left(x_{m+2c}\right)^{-1} - \left(x_{m+n+2c}\right)^{-1}}
        &= \abs{\frac{x_{m+2c} - x_{m+n+2c}}{x_{m+2c} \cdot x_{m+n+2c}}} \\
        &= \frac{\abs{x_{m+2c} - x_{m+n+2c}}}{\abs{x_{m+2c}} \abs{x_{m+n+2c}}} \\
        &< \frac{1}{2^{m+2c}} \cdot 2^{2c} \\
        &= \frac{1}{2^m} \ . \qedhere
    \end{align*}
\end{proof}

\begin{Corollary}[Division]
    $(x/y)_k := \left(x \cdot y^{-1}\right)_k$ is a \FCCS.
\end{Corollary}

\section{Extensionality}

For \FCCS{} to form a suitable model of computable real numbers, we need to introduce one more property: \textit{extensionality}. A function is extensional when it maps equal inputs to equal outputs, regardless of how the inputs are represented internally. In the context of \FCCS, this means that the function only concerns itself with what a sequence converges to, not the approximations along the way.

Most operations are extensional, including the arithmetic operations defined above. An example of a non-extensional function is the indexing operator $\iota : \FCCS \times \N \rightarrow \Q$, $\iota(x, n) = x_n$. If we let $x_n = 1/2^n$ and $y_n = -1/2^n$, then it is clear that $x = y$ but $\iota(x, n) \neq \iota(y, n)$. Intuitively, $\iota$ breaks the abstraction of a ``real number,'' exposing the Cauchy sequence representation underneath.

\begin{Definition}[Extensionality]
    A function $f : \FCCS \rightarrow \FCCS$ is extensional when $x = y$ implies $f(x) = f(y)$.
\end{Definition}

\begin{Proposition}
    Addition is extensional.
\end{Proposition}

\begin{proof}
    Let $x$, $y$, $z$ be arbitrary \FCCS{} where $x = y$.

    Then for any $n \in \N$,
    \begin{align*}
        \abs{ (x + z)_n - (y + z)_n }
        &= \abs{ (x_{n+1} + z_{n+1}) - (y_{n+1} + z_{n+1}) } \\
        &= \abs{ x_{n+1} - y_{n+1} } \\
        &\leq \frac{1}{2^{n+1}} \leq \frac{1}{2^n}
    \end{align*}

    Hence $x + z = y + z$ and addition is extensional in its first parameter.

    The dual $z + x = z + y$ can be proved in a similar way. Hence addition is extensional.
\end{proof}

The other operations can be verified using a similar argument. For brevity their extensionality will be assumed without proof.

\section{Exponentials and trigonometric functions}

With elementary arithmetic under our belt, we can turn our attention to more advanced functions such as the exponential map $\exp(x)$.

Recall that the exponential can be defined as

\[
    \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!}
    = 1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \frac{x^4}{4!} + \ldots
\]

In particular, this definition is useful because when $x$ is small, each term is exponentially smaller than the one before it. Hence the sequence of partial sums can be easily adapted to a \FCCS.

\begin{Proposition}[Exponential on $\abs{x} \leq 1$]
    Let $x$ be a rational number where $-1 \leq x \leq 1$. Then the exponential $\exp_{[-1,1]}(x)$, defined by
    \[
        \left[\exp_{[-1,1]}(x)\right]_n := \sum_{k=0}^{n+1} \frac{x^k}{k!},
    \]
    is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    It can be shown that $k! \leq 2^{k-1}$ for any $k \geq 1$.

    Then
    \begin{align*}
        \abs{\exp_{[-1,1]}(x)_m - \exp_{[-1,1]}(x)_{m+n}}
        &= \abs{\sum_{k=0}^{m+1} \frac{x^k}{k!} - \sum_{k=0}^{m+n+1} \frac{x^k}{k!}} \\
        &= \sum_{k=m+2}^{m+n+1} \frac{\abs{x^k}}{k!} \\
        &\leq \sum_{k=m+2}^{m+n+1} \frac{1}{k!} \\
        &\leq \sum_{k=m+2}^{m+n+1} \frac{1}{2^{k-1}} \\
        &= \sum_{k=m+1}^{m+n} \frac{1}{2^k} \\
        &< \frac{1}{2^m} \ . \qedhere
    \end{align*}
\end{proof}

We can then extend this definition to all rational numbers using the property $\exp(x + y) = \exp(x)\exp(y)$.

\begin{Corollary}[Exponential on $\Q$]
    Let $x$ be a rational number. Then the exponential $\exp(x)$, defined by
    \[
        \exp(x) := \begin{cases}
            \exp_{[-1,1]}(x) & -1 \leq x \leq 1 \\
            \exp(x/2)^2 & \textrm{otherwise}
        \end{cases}
    \]
    is a \FCCS.
\end{Corollary}

The trigonometric functions $\sin$, $\cos$, and $\tan$ can be defined in a similar way.

\begin{Proposition}[Trigonometric functions]
    Let $x$ be a rational number such that $-1 \leq x \leq 1$. Then the functions $\cos_{[-1,1]}(x)$ and $\sin_{[-1,1]}(x)$, defined by
    \[
        \left[\cos(x)_{[-1,1]}\right]_n
        := \sum_{k=0}^{n+1} \begin{cases}
            (-1)^{\floor{k/2}} \cdot \cfrac{x^k}{k!} & k\ \mathrm{even} \\
            0 & \mathrm{otherwise}
        \end{cases}
    \]
    and
    \[
        \left[\sin(x)_{[-1,1]}\right]_n
        := \sum_{k=0}^{n+1} \begin{cases}
            (-1)^{\floor{k/2}} \cdot \cfrac{x^k}{k!} & k\ \mathrm{odd} \\
            0 & \mathrm{otherwise}
        \end{cases}
    \]
    are \FCCS. Using the double angle formulae $\cos(2x) = 2 \cos^2 x - 1$ and $\sin(2x) = 2 \sin x \cos x$, these functions can be extended to all of $\Q$.

    $\tan(x)$ is then defined as $\sin(x) / \cos(x)$.
\end{Proposition}

\begin{proof}
    Observe that in the definitions of $\cos_{[-1,1]}$ and $\sin_{[-1,1]}$, the magnitude of each term is less than or equal to that of $\exp_{[-1,1]}$. Therefore we can use a similar argument to prove the convergence of the former.
\end{proof}

\section{Cantor's diagonal argument}

One important result in set theory is the uncountability of the real line: that is, there is no bijective mapping that relates $\N$ and $\R$. This result applies within constructive analysis as well, if we restrict the theorem to the computable reals (or \FCCS) instead.

A typical proof of this result uses Cantor's \textit{diagonal argument}. Given any sequence of (computable) real numbers, we can construct a number that is apart from every number in the list. Hence there cannot be an enumeration of all real numbers---if there were, then the diagonal argument would yield one that it missed. Furthermore, since this argument is constructive, it can be implemented as a computer program as well.

\begin{Proposition}[Cantor's diagonal argument]
    Let $x : \N \rightarrow \FCCS$ be a sequence of fast converging Cauchy sequences. Then there exists a \FCCS{} $\mathrm{cantor}(x)$ that is apart from every element in $x$.
\end{Proposition}

\begin{proof}
    We want to define a sequence of rational half-open intervals $[a_n, b_n) : \N \rightarrow \Q \times \Q$ such that the following properties hold:
    \begin{enumerate}[label=(A\arabic*)]
        \item $\abs{a_n - b_n} \leq 1/4^n$;
        \item $[a_{n+1}, b_{n+1}) \subset [a_n, b_n)$;
        \item $x_n \notin [a_n, b_n)$.
    \end{enumerate}
    for all $n \in \N$.

    Define $a_0 := [x_0]_0 - 2$, $b_0 := [x_0]_0 - 1$.
    For $n > 0$, suppose $[a_n, b_n)$ satisfies the properties above. Then define $a_{n+1}, b_{n+1}$ by the equation
    \[
        [a_{n+1}, b_{n+1}) := \begin{cases}
            [a_n, a_n + h_n) & [x_n]_{2n+2} \geq c_n \\
            [b_n - h_n, b_n) & [x_n]_{2n+2} < c_n
        \end{cases}
    \]
    where $c_n := (a_n + b_n) / 2$ and $h_n := (b_n - a_n) / 4 = 1/4^{n+1} = 1/2^{2n+2}$. Clearly (A1) and (A2) hold.

    Proof of (A3). Clearly $x_0 \notin [a_0, b_0)$. For $n > 0$, if $[x_n]_{2n+2} \geq c_n$ then $x_n \geq c_n - 1/2^{2n+2} = c_n - h_n = a_n + h_n$. Using a similar argument, if $[x_n]_{2n+2} < c_n$ then $x_n < b_n - h_n$. Hence (A3) holds.

    Now define $[\mathrm{cantor}(x)]_n := a_{\floor{n/2}}$. Then for any $m, n \in \N$,
    \begin{align*}
        \abs{[\mathrm{cantor}(x)]_m - [\mathrm{cantor}(x)]_{m+n}}
        &= \abs{a_{\floor{m/2}} - a_{\floor{(m+n)/2}}} \\
        &< \frac{1}{4^{\floor{m/2}}} \\
        &\leq \frac{1}{2^m}
    \end{align*}
    shows that $\mathrm{cantor}(x)$ is a \FCCS.

    By (A3) and Proposition~\ref{apartness}, this is apart from every element of $x$.
\end{proof}

\chapter{Implementation notes}

There are many different ways to implement fast converging Cauchy sequences on a computer. One aspect in which an implementation can differ is in how it treats the \textit{modulus function}.

A modulus function $\mu(\varepsilon) : \Q \rightarrow \N$ of a sequence returns the index of an element with error at most $\varepsilon$. In the context of \FCCS, we can simplify the definition by requiring $\varepsilon = 1/2^n$ for some natural $n$; then the modulus function can take this $n$ directly.

The \FCCS{} operations described thus far handle the modulus function implicitly---they shift their inputs such that $\mu$ is just the identity function. But it is possible to avoid shifting at all, and modify $\mu$ instead. In the attached code, the \texttt{Cauchy'} type implements this idea.

To decide which approach to use, the author ran benchmarks to compare the performance of each method. The two methods were tasked with evaluating two expressions:
\begin{itemize}
    \item A ``simple'' expression, $(e \cdot e + e) / e$, where $e$ is Euler's constant; and
    \item A ``complicated'' expression, which is $e$ added to itself 50 times.
\end{itemize}

Both expressions were evaluated within an error of $1/2^{50}$. The benchmarks were run using the \texttt{criterion} library on an Intel Core i7 laptop.

\begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{Benchmark} &
    \multicolumn{2}{c}{No modulus} &
    \multicolumn{2}{c}{With modulus} \\
    \cmidrule(l){2-5}
    & Mean (ms) & Std. dev. ($\mu$s) & Mean (ms) & Std. dev. ($\mu$s) \\
    \midrule
    Simple & 3.41 & 11.1 & 3.51 & 264 \\
    Complicated & 369 & 85.0 & 706 & 598 \\
    \bottomrule
\end{tabular}

It is interesting to note that while there is little difference in the ``simple'' benchmark, the no-modulus-function method is almost twice as fast in the ``complicated'' benchmark. This is likely because the modulus function variant involves two function calls instead of one. Over many arithmetic operations, these extra calls can cause a significant slowdown.

\bibliographystyle{abbrv}
\bibliography{All}

\end{document}
