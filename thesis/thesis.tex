\documentclass[leqno]{report}
\input{base.tex}
\begin{document}
\author{Chris Wong}
\title{Computable analysis via fast converging Cauchy sequences}
\maketitle

\chapter*{Preface}

In classical mathematics, one often proves a result without constructing the object in question. The archetypal \textit{non-constructive} proof would be the result by \citet{jarden1953simple}:

\begin{Theorem}
    A power of an irrational number to an irrational exponent may be rational.
\end{Theorem}

\begin{proof}
    $\sqrt{2}^{\sqrt{2}}$ is either rational or irrational. If it is rational, our statement is proved. If it is irrational, $(\sqrt{2}^{\sqrt{2}})^{\sqrt{2}} = (\sqrt{2})^2 = 2$ proves our statement.
\end{proof}

What makes this non-constructive is the claim that we can decide whether or not any number is rational. While the proof handles both cases, it does not show which of these is correct.\footnote{It can be shown that $\sqrt{2}^{\sqrt{2}}$ is indeed irrational by the Gelfond--Schneider theorem; this can then be used to complete the original proof, provided there exists a constructive proof of Gelfond--Schneider as well.} The crux of the issue is the \textit{law of excluded middle}, which states that $A \vee \neg A$ for \emph{any} proposition $A$. Constructive mathematicians reject this law in general, as it yields no information on the construction of either $A$ or $\neg A$.

(Note that this law may still hold in specific cases. For example, given any natural number $n$ we can still determine which of $n = 1$ or $n \neq 1$ is true. A predicate which has this property is called \textit{decidable}.)

As with many areas of mathematics, the theory preceded its application. Constructive proofs can be seen as computer programs, a fact known as the Curry--Howard correspondence \citep{wadler2015}. In particular, the study of \textit{constructive analysis}---real numbers, convergence and continuity in a constructive setting---can often lead to novel, proven-correct numerical algorithms. It is this relationship between constructive analysis and computation that will be explored in this thesis.

Past treatments, such as the aptly named \textit{Constructive Analysis} \citep{bishop1985constructive}, have often favored a more theoretical approach; using a definition of the reals that is mathematically simple but computationally inefficient. The typical definition of Cauchy sequences includes a \textit{modulus function} $\mu$, which maps an error threshold $\varepsilon$ to an index $n := \mu(\varepsilon)$. A na√Øve implementation of Cauchy sequences would then comprise two steps: compute the index $n$, then compute the element $x_n$ at said index. But as we will see later, the overhead of separating these steps can eclipse the running time of the steps themselves.

We propose an alternative system based on \textit{fast converging Cauchy sequences} (\FCCS), which combines the modulus and indexing functions into a single operation. Furthermore, since a \FCCS{} converges exponentially to its limit point, to compute another digit of precision only requires a handful of extra operations. The theory of \FCCS{} then becomes a practical, efficient implementation of exact real arithmetic.

In Chapter 1 we discuss different representations of real numbers, and the advantages and disadvantages thereof. In Chapter 2 we introduce the definition of \FCCS, along with associated operations and proofs. Chapter 3 evaluates the performance of this system, and justifies the requirement of fast convergence. Finally, the Appendix comprises a sample implementation of \FCCS{} written in the Haskell programming language.

\section{Computability}

A number is \textit{computable} when it can be approximated to arbitrary precision by a (finite, terminating) computer program. The program itself can be written as either a Turing machine, a lambda calculus expression, or in any other equivalent model of computation. A function $\R \rightarrow \R$ is computable when any finite prefix of its result can be computed from a finite prefix of its argument. For the purposes of this project, we will consider all real numbers to be computable unless stated otherwise.

\chapter{Representations of real numbers}

There are many possible ways to represent real numbers in a computational setting. While this thesis will primarily cover Cauchy sequences, other common representations are worth mentioning as well.

\section{Decimal expansion}

One naive approach is to represent a real number by a sequence of digits in some base $b$. This base can be any integer greater than 1, but is usually set to 2 or 10. In the latter case, this is our usual \textit{decimal expansion}.

Formally, a non-negative real number $r = a_0 . a_1 a_2 a_3 \ldots$ is defined as the sum
\[
    r = \sum_{k=0}^\infty \frac{a_k}{b^k},
\]
where $a_0$ forms the integer part, and $0 \leq a_1, a_2, \ldots < b$ form the fractional part. If the expansion ends in zeroes, then it is said to be \textit{finite}.

While this representation is easy to understand, it has drawbacks that make it hard to use in practice. First, it is not unique: the decimal expansions $0.999\ldots$ and $1.000\ldots$ are two different representations of 1. We can preserve uniqueness by removing sequences that end in nines, but that leads to issues elsewhere. For example, suppose $x = 0.090909\ldots$ and $y = 0.909090\ldots$ are real numbers. While we could be tempted to conclude that $x + y = 1$, this may not be the case. It may turn out that after a million digits, $x$ contains a `0' instead of a `9', resulting in a sum that is less than 1. In fact, it would take an infinite number of steps to be sure whether the leading digit of $x + y$ is `1' or `0'.

This problem touches on a fundamental weakness of the decimal representation. Since computing more digits can only \emph{increase} the approximation, not decrease it, each term then forms a lower bound to the true value. In other words, if we know that the number $x$ starts with the digit $1$, then we also know that $x \geq 1$. But as the addition example shows, it is undecidable in general whether or not a real number satisfies such a bound. For this reason, decimal expansions are not suited to computable analysis.

\section{Cauchy sequences}

An alternative formulation is \textit{Cauchy sequences}, a sequence of rational numbers which become ``arbitrarily close'' to each other as the sequence progresses. In the classical definition, a sequence is Cauchy when for any bound $\varepsilon > 0$, there is an index $N$ after which any pair of points differ by less than $\varepsilon$.

For the purposes of computation, we also require that the sequence is \textit{fast converging}; that is, the sequence converges exponentially to its limit point. Formally, we require that there is some fixed \textit{modulus function} $\mu$ such that for any natural numbers $m$ and $n$, $\abs{x_{\mu(m)} - x_{\mu(m)+n}} < 1/2^m$. This restriction lets us find a suitable approximation in reasonable time.

Other formulations also require the denominator of each element to be a successive power of $2$. This restriction simplifies the implementation, at the cost of further complicating the theory. The system designed by \citet{gowland2000correctness} uses this technique. To avoid duplicating their work, we do not apply this idea in our implementation.

\section{Continued fractions}

A \textit{(simple) continued fraction} is a possibly-infinite sequence of integers $[a_0; a_1, a_2, \ldots, a_n]$, where $a_1, a_2, \ldots$ are positive, which represents the real number
\[
    a_0 + \cfrac{1}{a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n}}}}
    \ .
\]

This representation has many useful properties. It is unique \citep{proofwiki2013}; it is rational if and only if the sequence is finite; the sequences for $e$, $\sqrt{n}$ for any integer $n$, and the golden ratio $\phi$ are easy to compute. Compared to Cauchy sequences it has two drawbacks: the underlying theory is not as easy to understand; and its implementations are slower in practice.

\chapter{Fast converging Cauchy sequences}

For the remainder of this thesis, we will discuss \textit{fast converging Cauchy sequences} (\FCCS). These differ from the usual definition in that they must converge exponentially to their limit point. Despite this restriction, \FCCS{} are equivalent to Cauchy sequences---at least in a mathematical sense---and hence form a model for the (computable) real numbers.

\begin{Definition}[\citet{simpson2009subsystems}]
    A fast converging Cauchy sequence (\FCCS) $x : \N \rightarrow \Q$ has the property that
    \[ \fa{m, n \in \N}{ \abs{x_m - x_{m+n}} < \frac{1}{2^m} } \ . \]
\end{Definition}

\begin{Proposition}
    Every fast converging Cauchy sequence is a Cauchy sequence, and vice versa.
\end{Proposition}

\begin{proof}
    ($\Rightarrow$) Let $x$ be a \FCCS, $\varepsilon > 0$ arbitrary. Let $m$ be the least natural number such that $1/2^m \leq \varepsilon$. Then for all $n \in \N$, $\abs{x_m - x_{m+n}} < 1/2^m \leq \varepsilon$.

    ($\Leftarrow$) Let $x$ be a Cauchy sequence. Define $y_m := x_{N(1/2^m)}$ where $N(\varepsilon)$ is the index where all subsequent points differ by less than $\varepsilon$. Then $y$ is a \FCCS.
\end{proof}

\section{Arithmetic and closure properties}

With Cauchy sequences, we define arithmetic operations simply by working pointwise on each element. But with \FCCS, we need to do extra work to preserve the fast convergence property. Take addition, for example: when two approximations are added together, their errors are added as well. In general, this means that the error of the result is \textit{twice} that of the inputs. Hence the addition routine must shift the inputs one step to the right, halving the error to compensate.

\begin{Proposition}[Addition]
    \label{add}
    Let $x, y$ be \FCCS.

    The sum $x + y$, defined by $(x+y)_k := x_{k+1} + y_{k+1}$, is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    Then
    \begin{align*}
        \abs{(x+y)_m - (x+y)_{m+n}}
        &= \abs{(x_{m+1} + y_{m+1}) - (x_{m+n+1} + y_{m+n+1})} \\
        &= \abs{x_{m+1} - x_{m+n+1} + y_{m+1} - y_{m+n+1}} \\
        &\leq \abs{x_{m+1} - x_{m+n+1}} + \abs{y_{m+1} - y_{m+n+1}} \\
        &< \frac{1}{2^{m+1}} + \frac{1}{2^{m+1}} \\
        &= \frac{1}{2^m}
        \ . \qedhere
    \end{align*}
\end{proof}

\begin{Proposition}[Negation]
    \label{neg}
    If $x$ is a \FCCS, then $(-x)_k := -(x_k)$ is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    Then
    \[
        \abs{(-x)_m - (-x)_{m+n}}
        = \abs{-x_m - (-x_{m+n})}
        = \abs{x_m - x_{m+n}}
        < \frac{1}{2^m}
        \ . \qedhere
    \]
\end{proof}

\begin{Corollary}[Subtraction]
    $(x - y)_k := (x + (-y))_k = x_{k+1} - y_{k+1}$ is a \FCCS.
\end{Corollary}

Multiplication is more subtle. Unlike with addition, the shift under multiplication is not constant. A larger input would necessitate a larger shift, since the multiplication operation scales the error up by a corresponding amount.

For example, consider the sequences $x := (1, \frac{1}{2}, \frac{1}{4}, \ldots)$ and $y := (16, 16, \ldots)$. It can be shown that $x$ and $y$ are \FCCS; but if we multiply them pointwise, the result $(16, 8, 4, \ldots)$ is not fast converging. To recover fast convergence, this sequence must be shifted not once, but $\log_2 16 = 4$ times.

\begin{Proposition}[Multiplication]
    $(xy)_k := x_{a(k)} \cdot y_{b(k)}$ is a \FCCS, where
    \begin{align*}
        a(k) &= k + 1 + \phi(y_0) \\
        b(k) &= k + 1 + \phi(x_0) \\
        \phi(z) &= \max \menge{ 0,\, \ceil{\log_2(\abs{z} + 1)} }
        \ .
    \end{align*}
\end{Proposition}

\begin{proof}
    Given $m, n \in \N$, we need to choose $a,\, b : \N \rightarrow \N$ such that
    \begin{align*}
        \abs{(xy)_m - (xy)_{m+n}}
        &= \abs{x_{a(m)} y_{b(m)} - x_{a(m+n)} y_{b(m+n)}} \\
        &= \abs{x_{a(m)} y_{b(m)} - x_{a(m)} y_{b(m+n)} + x_{a(m)} y_{b(m+n)} - x_{a(m+n)} y_{b(m+n)}} \\
        &\leq \abs{x_{a(m)} y_{b(m)} - x_{a(m)} y_{b(m+n)}} + \abs{x_{a(m)} y_{b(m+n)} - x_{a(m+n)} y_{b(m+n)}} \\
        &= \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}} + \abs{y_{b(m+n)}}\abs{x_{a(m)} - x_{a(m+n)}} \\
        &< \frac{1}{2^m}
        \ .
    \end{align*}

    For this inequality to hold, it is sufficient to show that
    \[ \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}} < \frac{1}{2^{m+1}} \]
    and
    \[ \abs{y_{b(m+n)}}\abs{x_{a(m)} - x_{a(m+n)}} < \frac{1}{2^{m+1}} \ . \]

    To complete the proof, we observe that $\abs{x_{a(m)}} < \abs{x_0} + 1$ and $\phi(x_0) \geq \log_2(\abs{x_0} + 1)$. Using these facts:
    \begin{align*}
        \abs{x_{a(m)}}\abs{y_{b(m)} - y_{b(m+n)}}
        &< (\abs{x_0} + 1) \cdot \frac{1}{2^{b(m)}} \\
        &= \frac{\abs{x_0} + 1}{2^{m + 1 + \phi(x_0)}} \\
        &\leq \frac{\abs{x_0} + 1}{2^{m + 1 + \log_2(\abs{x_0} + 1)}} \\
        &= \frac{\abs{x_0} + 1}{2^{m + 1} \cdot (\abs{x_0} + 1)} \\
        &= \frac{1}{2^{m + 1}}
        \ .
    \end{align*}

    Similarly,
    \begin{align*}
        \abs{y_{a+b}}\abs{x_a - x_{a+b}}
        &< (\abs{y_0} + 1) \cdot \frac{1}{2^{m + 1 + \log_2(\abs{y_0} + 1)}} \\
        &= \frac{1}{2^{m + 1}}
        \ . \qedhere
    \end{align*}
\end{proof}

To derive the final arithmetic operation, division, we must introduce the idea of (in-)equality first.

\section{Ordering}

Constructive analysis presents unique challenges in defining a total order on the reals. Without additional axioms, $a < b$ and $a \geq b$ are not full inverses; we are forced to favor one over the other. Many classical concepts, such as the sign function or equality (!), fall apart in a constructive setting. In this section we will define these concepts in terms of \FCCS{}, and explore the consequences in terms of computability.

\begin{Definition}[Equality]
    A pair of \FCCS{} $x$ and $y$ are equal, written $x = y$, when
    \[
        \fa{n \in \N}{\abs{x_{n+1} - y_{n+1}} \leq \frac{1}{2^n}} \ .
    \]
    $x$ is not equal to $y$, written $x \neq y$, when $\neg (x = y)$.
\end{Definition}

Note that in the definition of equality, we compare the terms $x_{n+1}$ and $y_{n+1}$, not $x_n$ and $y_n$. This shifting can be justified with an example. Suppose $x$ and $y$ are sequences where $x := (0, \frac{1}{8}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \ldots)$ and $y := (1, \frac{7}{8}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \ldots)$. It can be shown that $x$ and $y$ are fast converging. Clearly $x$ and $y$ should be equal. But since $\abs{x_1 - y_1} = 3/4 > 1/2^1$, without the shift we would be forced to deem them unequal. With the shift in place, the modified bound $\abs{x_1 - y_1} \leq 1/2^0$ holds as required.

An important result of constructive analysis is that equality of real numbers is undecidable---that is, there is no algorithm that can determine whether two arbitrary numbers are equal or not. This is a consequence of the Halting Problem \citep{turing1936computable}: since the execution of a computer program can be encoded as a real number, deciding equality would solve the Halting Problem as well.

\begin{Proposition} \label{equality_undecidable}
    Equality on \FCCS{} is undecidable.
\end{Proposition}

\begin{proof}
    Let $P$ be an arbitrary computer program. Define the \FCCS{} $x$ as follows.

    Given an arbitrary index $n$, run the program $P$ for at most $n$ steps. If the program terminates in $p \leq n$ steps, then define $x_n := 1/2^p$. Otherwise, set $x_n := 1/2^n$. In other words, $x \rightarrow 1/2^p \neq 0$ if and only if $P$ terminates.

    Suppose there is an algorithm for deciding whether $x = 0$. Then we also have an algorithm that decides whether an arbitrary program terminates. Hence the Halting Problem is solved, and we have a contradiction.
\end{proof}

\begin{Corollary}
    The sign function
    \[
        \mathrm{sgn}(x) := \begin{cases}
            -1 & x < 0 \\
            0 & x = 0 \\
            1 & x > 0
        \end{cases}
    \]
    is not computable.
\end{Corollary}

\begin{proof}
    If the sign function was computable, then we could determine whether $x$ and $y$ are equal by evaluating $\mathrm{sgn}(x - y)$. This contradicts \cref{equality_undecidable}.
\end{proof}

In particular, constructive analysis forces a distinction between constructive and non-constructive inequality. A statement that two numbers are \textit{not equal} is just a negation of the statement that they are equal. But for two numbers to be \textit{apart}, we must provide an index after which the sequences will never come closer together. While it is easy to show that apartness implies not-equality, the converse requires an additional axiom: Markov's principle.

\begin{Definition}[Ordering]
    We say that $x < y$ when there exists $c \in \N$ such that
    \[ \fa{k \in \N}{y_{c+k} - x_{c+k} > \frac{1}{2^c}} \ . \]

    $x \leq y$ is equivalent to $\neg (y < x)$.
\end{Definition}

\begin{Definition}[Apartness]
    $x$ and $y$ are apart when $x < y$ or $y < x$.
\end{Definition}

\begin{Lemma}[Apartness from zero] \label{apartness}
    Suppose there exists $n \in \N$ such that $\abs{x_n} > 1/2^n$.
    Then either $x < 0$ or $x > 0$.
\end{Lemma}

\begin{proof}
    If $\abs{x_n} > 1/2^n$, then either $x_n > 1/2^n$ or $-x_n > 1/2^n$. Assume the former case.

    Applying the definition of a \FCCS, for all $k \in \N$
    \begin{align*}
        \abs{x_n - x_{n+k}} < \frac{1}{2^n}
        \enspace&\Rightarrow\enspace x_n - x_{n+k} < \frac{1}{2^n} \\
        \enspace&\Rightarrow\enspace x_n - \frac{1}{2^n} < x_{n+k} \ .
    \end{align*}

    Let $c \geq n$ be the least natural number such that $1/2^c \leq x_n - 1/2^n$. Then
    \[ x_{c+k} > x_n - \frac{1}{2^n} \geq \frac{1}{2^c} \]

    Since $k$ is arbitrary, this shows that $x > 0$. With a similar argument, it can be shown that the other case implies $x < 0$.
\end{proof}

\begin{Definition}[Markov's principle]
    Suppose $x : \N \rightarrow X$ is a sequence, and $P$ is a decidable predicate on $X$ (that is, as noted in the preface, $P(a) \vee \neg P(a)$ for any $a \in X$). Then
    \[
        \neg \forall n\ P(x_n)
        \ \Rightarrow\ \exists n\ \neg P(x_n) \ .
    \]
\end{Definition}

\begin{Proposition}[Apartness]
    If $x \neq y$, then by Markov's principle $x$ is apart from $y$.
\end{Proposition}

\begin{proof}
    Let $z := x - y$. Applying Markov's principle,
    \[
        \neg \left( \fa{n \in \N}{\abs{x_{n+1} - y_{n+1}} \leq \frac{1}{2^n}} \right)
        \quad \Rightarrow \quad
        \ex{n \in \N}{\abs{x_{n+1} - y_{n+1}} > \frac{1}{2^n}} \ .
    \]
    Hence $\abs{z_n} = \abs{x_{n+1} - y_{n+1}} > 1/2^n$, and by \cref{apartness}, either $z < 0$ or $z > 0$. From that we conclude $x < y$ or $x > y$ respectively. Therefore $x$ and $y$ are apart.
\end{proof}

Operationally, Markov's principle represents an unbounded search. The operations described thus far have all been bounded. Even in the case of multiplication, to compute $(x \cdot y)_n$ we only need to check four elements: $x_0$, $y_0$, $x_{a(n)}$, and $y_{b(n)}$. While the indices $a$ and $b$ can be large, they are determined solely by the initial approximations $x_0$ and $y_0$. But when finding evidence of apartness, we must inspect each element of the sequence until this evidence is found. There is no guarantee as to how many elements we need to inspect, other than that it will be found eventually. This unboundedness is what justifies the use of Markov's principle.

For example, consider the sequence $x$ whose first $N$ terms are zero, with the rest defined as $1/2^N$. Then $x$ is not equal to zero, but it would take $N$ steps to show that it is apart. Furthermore, if the input were in fact equal, then the algorithm would search forever for evidence that does not exist. Since equality of real numbers is undecidable, there cannot be an algorithm that has a better worst case.

Finally, we note that though the law of trichotomy is undecidable, the related notions of absolute value and minimum/maximum are not.

\begin{Proposition}[Absolute value]
    If $x$ is a \FCCS, then $\abs{x}_n := \abs{x_n}$ is a \FCCS.
\end{Proposition}

\begin{proof}
    For any $m, n \in \N$,
    \begin{align*}
        \abs{\abs{x}_m - \abs{x}_{m+n}}
        &= \abs{\abs{x_m} - \abs{x_{m+n}}} \\
        &\leq \abs{x_m - x_{m+n}} \\
        &< \frac{1}{2^m}
    \end{align*}
    by the reverse triangle inequality.
\end{proof}

\begin{Lemma}[Maximum as absolute value] \label{max_abs}
    If $x$ and $y$ are rational numbers, then
    \[
        \max(x, y) = \frac{x + y + \abs{x - y}}{2} \ .
    \]
\end{Lemma}

\begin{proof}
    If $x \geq y$, then $\abs{x - y} = x - y$. Hence
    \[
        \frac{x + y + (x - y)}{2} = x = \max(x, y) \ .
    \]
    On the contrary, if $x < y$ then $\abs{x - y} = -(x - y)$. Hence
    \[
        \frac{x + y - (x - y)}{2} = y = \max(x, y) \ . \qedhere
    \]
\end{proof}

\begin{Proposition}[Maximum and minimum]
    Let $x$ and $y$ be \FCCS. Then the maximum, defined by $\max(x, y)_n := \max\left(x_{n+1}, y_{n+1}\right)$, is a \FCCS. Furthermore, the minimum $\min(x, y) := -\max(-x, -y)$ is also a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    Then by \cref{max_abs},
    \begin{align*}
        \abs{\max(x, y)_m - \max(x, y)_{m+n}}
        &= \abs{\max(x_{m+1}, y_{m+1}) - \max(x_{m+n+1}, y_{m+n+1})} \\
        &= \frac{1}{2} \abs{x_{m+1} + y_{m+1} + \abs{x_{m+1} - y_{m+1}} - x_{m+n+1} - y_{m+n+1} - \abs{x_{m+n+1} - y_{m+n+1}}} \\
        &\leq \frac{1}{2} \left( \abs{x_{m+1} - x_{m+n+1}} + \abs{y_{m+1} - y_{m+n+1}} + \abs{x_{m+1} - x_{m+n+1}} + \abs{y_{m+1} - y_{m+n+1}} \right) \\
        &< \frac{1}{2} \left( \frac{1}{2^{m+1}} + \frac{1}{2^{m+1}} + \frac{1}{2^{m+1}} + \frac{1}{2^{m+1}} \right) \\
        &= \frac{1}{2^m} \ .
    \end{align*}

    The fast convergence of $\min(x, y)$ then follows from \cref{neg}.
\end{proof}

\section{Division}

To derive the reciprocal, and with it division, observe that $x^{-1}$ is defined only when $x$ is not zero. Hence intuitively, some constructive evidence that a number is not zero should assist in computing its reciprocal. Indeed this is this the case: the lower bound provided by apartness translates to an upper bound on the result.

\begin{Proposition}[Reciprocal]
    Let $x$ be a \FCCS{} where $x \neq 0$. Then $\left(x^{-1}\right)_k := \left(x_{k+2c}\right)^{-1}$ is a \FCCS, where $c$ is defined as in \cref{apartness}.
\end{Proposition}

\begin{proof}
    By the definition of a \FCCS, we know that
    \[
        \abs{x_{m+2c} - x_{m+n+2c}} < \frac{1}{2^{m+2c}} \ ;
    \]

    And by \cref{apartness}, we know that
    \begin{align*}
        \abs{x_{m+2c}} > \frac{1}{2^c} \enspace\wedge\enspace
        \abs{x_{m+n+2c}} > \frac{1}{2^c}
        \enspace&\Rightarrow\enspace
        \abs{x_{m+2c}} \abs{x_{m+n+2c}} > \frac{1}{2^{2c}} \\
        &\Rightarrow\enspace
        \frac{1}{\abs{x_{m+2c}} \abs{x_{m+n+2c}}} < 2^{2c} \ .
    \end{align*}

    Therefore
    \begin{align*}
        \abs{\left(x_{m+2c}\right)^{-1} - \left(x_{m+n+2c}\right)^{-1}}
        &= \abs{\frac{x_{m+2c} - x_{m+n+2c}}{x_{m+2c} \cdot x_{m+n+2c}}} \\
        &= \frac{\abs{x_{m+2c} - x_{m+n+2c}}}{\abs{x_{m+2c}} \abs{x_{m+n+2c}}} \\
        &< \frac{1}{2^{m+2c}} \cdot 2^{2c} \\
        &= \frac{1}{2^m} \ . \qedhere
    \end{align*}
\end{proof}

\begin{Corollary}[Division]
    $(x/y)_k := \left(x \cdot y^{-1}\right)_k$ is a \FCCS.
\end{Corollary}

\section{Extensionality}

\todo[inline]{Explain the role of extensionality/well-definedness re modelling the computable reals}

For \FCCS{} to form a suitable model of computable real numbers, we need to introduce one more property: \textit{extensionality}. A function is extensional when it maps equal inputs to equal outputs, regardless of how the inputs are represented internally. In the context of \FCCS, this means that the function only concerns itself with what a sequence converges to, not the approximations along the way.

Most operations are extensional, including the arithmetic operations defined above. An example of a non-extensional function is the indexing operator $\iota : \FCCS \times \N \rightarrow \Q$, $\iota(x, n) := x_n$. If we let $x_n = 1/2^n$ and $y_n = -1/2^n$, then it is clear that $x = y$ but $\iota(x, n) \neq \iota(y, n)$. Intuitively, $\iota$ breaks the abstraction of a ``real number,'' exposing the Cauchy sequence representation underneath.

\begin{Definition}[Extensionality]
    A function $f : \FCCS \rightarrow \FCCS$ is extensional when $x = y$ implies $f(x) = f(y)$.
\end{Definition}

\begin{Proposition}
    Addition is extensional.
\end{Proposition}

\begin{proof}
    Let $x$, $y$, $z$ be arbitrary \FCCS{} where $x = y$.

    Then for any $n \in \N$,
    \begin{align*}
        \abs{ (x + z)_n - (y + z)_n }
        &= \abs{ (x_{n+1} + z_{n+1}) - (y_{n+1} + z_{n+1}) } \\
        &= \abs{ x_{n+1} - y_{n+1} } \\
        &\leq \frac{1}{2^{n+1}} \leq \frac{1}{2^n}
    \end{align*}

    Hence $x + z = y + z$ and addition is extensional in its first parameter.

    The dual $z + x = z + y$ follows from commutativity. Hence addition is extensional.
\end{proof}

The other operations can be verified using a similar argument. For brevity their extensionality will be assumed without proof.

\section{Exponentials and trigonometric functions}

With elementary arithmetic under our belt, we can turn our attention to more advanced functions such as the exponential map $\exp(x)$.

Recall that the exponential can be defined as

\[
    \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!}
    = 1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \frac{x^4}{4!} + \ldots
\]

In particular, this definition is useful because when $x$ is small, each term is exponentially smaller than the one before it. Hence the sequence of partial sums can be easily adapted to a \FCCS.

\begin{Proposition}[Exponential on $\abs{x} \leq 1$]
    Let $x$ be a \emph{rational} number where $-1 \leq x \leq 1$. Then the exponential $\exp_{[-1,1]}(x)$, defined by
    \[
        \left[\exp_{[-1,1]}(x)\right]_n := \sum_{k=0}^{n+1} \frac{x^k}{k!},
    \]
    is a \FCCS.
\end{Proposition}

\begin{proof}
    Let $m, n \in \N$ be arbitrary.

    It can be shown that $k! \leq 2^{k-1}$ for any $k \geq 1$.

    Then
    \begin{align*}
        \abs{\exp_{[-1,1]}(x)_m - \exp_{[-1,1]}(x)_{m+n}}
        &= \abs{\sum_{k=0}^{m+1} \frac{x^k}{k!} - \sum_{k=0}^{m+n+1} \frac{x^k}{k!}} \\
        &= \sum_{k=m+2}^{m+n+1} \frac{\abs{x^k}}{k!} \\
        &\leq \sum_{k=m+2}^{m+n+1} \frac{1}{k!} \\
        &\leq \sum_{k=m+2}^{m+n+1} \frac{1}{2^{k-1}} \\
        &= \sum_{k=m+1}^{m+n} \frac{1}{2^k} \\
        &< \frac{1}{2^m} \ . \qedhere
    \end{align*}
\end{proof}

We can then extend this definition to all rational numbers using the property $\exp(x + y) = \exp(x)\exp(y)$.

\begin{Corollary}[Exponential on $\Q$]
    Let $x$ be a rational number. Then the exponential $\exp(x)$, defined by
    \[
        \exp(x) := \begin{cases}
            \exp_{[-1,1]}(x) & -1 \leq x \leq 1 \\
            \exp(x/2)^2 & \textrm{otherwise}
        \end{cases}
    \]
    is a \FCCS.
\end{Corollary}

\begin{proof}
    Since $x$ is rational, it is decidable whether it lies in the interval $[-1, 1]$. Moreover, the Archimedean property $\exp(x)$ is guaranteed to terminate. The fast convergence of $\exp(x)$ then follows.
\end{proof}

The trigonometric functions $\sin$, $\cos$, and $\tan$ can be defined in a similar way.

\begin{Proposition}[Trigonometric functions]
    Let $x$ be a rational number such that $-1 \leq x \leq 1$. Then the functions $\cos_{[-1,1]}(x)$ and $\sin_{[-1,1]}(x)$, defined by
    \[
        \left[\cos(x)_{[-1,1]}\right]_n
        := \sum_{k=0}^{n+1} \begin{cases}
            (-1)^{\floor{k/2}} \cdot \cfrac{x^k}{k!} & k\ \mathrm{even} \\
            0 & \mathrm{otherwise}
        \end{cases}
    \]
    and
    \[
        \left[\sin(x)_{[-1,1]}\right]_n
        := \sum_{k=0}^{n+1} \begin{cases}
            (-1)^{\floor{k/2}} \cdot \cfrac{x^k}{k!} & k\ \mathrm{odd} \\
            0 & \mathrm{otherwise}
        \end{cases}
    \]
    are \FCCS. Using the double angle formulae $\cos(2x) = 2 \cos^2 x - 1$ and $\sin(2x) = 2 \sin x \cos x$, these functions can be extended to all of $\Q$.

    $\tan(x)$ is then defined as $\sin(x) / \cos(x)$.
\end{Proposition}

\begin{proof}
    Observe that in the definitions of $\cos_{[-1,1]}$ and $\sin_{[-1,1]}$, the magnitude of each term is less than or equal to that of $\exp_{[-1,1]}$. Therefore we can use a similar argument to prove the convergence of the former.
\end{proof}

\section{Cantor's diagonal argument}

One important result in set theory is the uncountability of the real line: that is, there is no bijective mapping that relates $\N$ and $\R$. This result applies within constructive analysis as well, if we restrict the theorem to the computable reals (or \FCCS) instead.

A typical proof of this result uses Cantor's \textit{diagonal argument}. Given any sequence of (computable) real numbers, we can construct a number that is apart from every number in the list. Hence there cannot be an enumeration of all real numbers---if there were, then the diagonal argument would yield one that it missed. The computable reals are then an example of a set that is \textit{countable} but not \textit{enumerable}: every program can be written as a natural number, but there is no algorithm that can list every terminating program.

Furthermore, since this argument is constructive, it can be implemented as a computer program as well. The Haskell implementation includes an enumeration of the rationals, which on application of this argument results in an arbitrary irrational number.

\begin{Proposition}[Cantor's diagonal argument]
    Let $x : \N \rightarrow \FCCS$ be a sequence of fast converging Cauchy sequences such that $x = (x^{(0)}, x^{(1)}, \ldots)$. Then there exists a \FCCS{} $\mathrm{cantor}(x)$ that is apart from every element in $x$.
\end{Proposition}

\begin{proof}
    We want to define a sequence of rational half-open intervals $[a_n, b_n) : \N \rightarrow \Q \times \Q$ such that the following properties hold:
    \begin{enumerate}[label=(C\arabic*)]
        \item $\abs{a_n - b_n} \leq 1/4^n$;
        \item $[a_{n+1}, b_{n+1}) \subset [a_n, b_n)$;
        \item $x^{(n)} \notin [a_n, b_n)$.
    \end{enumerate}
    for all $n \in \N$.

    Define $a_0 := x^{(0)}_0 - 2$, $b_0 := x^{(0)}_0 - 1$.
    For $n > 0$, suppose $[a_n, b_n)$ satisfies the properties above. Then define $a_{n+1}, b_{n+1}$ by the equation
    \[
        [a_{n+1}, b_{n+1}) := \begin{cases}
            [a_n, a_n + h_n) & x^{(n)}_{2n+2} \geq c_n \\
            [b_n - h_n, b_n) & x^{(n)}_{2n+2} < c_n
        \end{cases}
    \]
    where $c_n := (a_n + b_n) / 2$ and $h_n := (b_n - a_n) / 4 = 1/4^{n+1} = 1/2^{2n+2}$. Clearly (C1) and (C2) hold, as we shrink the interval by a factor of $1/4$ on each step.

    Proof of (C3). Clearly $x^{(0)} \notin [a_0, b_0)$. For the inductive case, if $x^{(n)}_{2n+2} \geq c_n$ then $x^{(n)} \geq c_n - 1/2^{2n+2} = c_n - h_n = a_n + h_n$. Using a similar argument, if $x^{(n)}_{2n+2} < c_n$ then $x^{(n)} < b_n - h_n$. Hence (C3) holds.

    Now define $[\mathrm{cantor}(x)]_n := a_{\floor{n/2}}$. Then for any $m, n \in \N$,
    \begin{align*}
        \abs{[\mathrm{cantor}(x)]_m - [\mathrm{cantor}(x)]_{m+n}}
        &= \abs{a_{\floor{m/2}} - a_{\floor{(m+n)/2}}} \\
        &< \frac{1}{4^{\floor{m/2}}} \\
        &\leq \frac{1}{2^m}
    \end{align*}
    shows that $\mathrm{cantor}(x)$ is a \FCCS.

    By (C3) and \cref{apartness}, this is apart from every element of $x$.
\end{proof}

\chapter{Implementation notes}

There are many different ways to implement fast converging Cauchy sequences on a computer. One aspect in which an implementation can differ is in how it treats the \textit{modulus function}.

A modulus function $\mu(\varepsilon) : \Q \rightarrow \N$ of a sequence returns the index of an element with error at most $\varepsilon$. In the context of \FCCS, we can simplify the definition by requiring $\varepsilon = 1/2^n$ for some natural $n$; then the modulus function can take this $n$ directly.

The \FCCS{} operations described thus far handle the modulus function implicitly---they shift the sequence such that $\mu$ is just the identity function. But it is possible to avoid shifting at all, and modify $\mu$ instead. In the attached code, the \texttt{Cauchy'} type implements this idea.

To decide which approach to use, the author ran benchmarks to compare the performance of each method. The two methods were tasked with evaluating two expressions:
\begin{itemize}
    \item A ``simple'' expression, $(e \cdot e + e) / e$, where $e = \exp 1$ is Euler's constant; and
    \item A ``complicated'' expression, which is $e$ added to itself 50 times.
\end{itemize}

Both expressions were evaluated within an error of $1/2^{50}$. The benchmarks were run using the \texttt{criterion} library on an Intel Core i7 laptop.

\begin{center} \begin{tabular}{lllll}
    \toprule
    \multirow{2}{*}{Benchmark} &
    \multicolumn{2}{c}{No modulus} &
    \multicolumn{2}{c}{With modulus} \\
    \cmidrule(l){2-5}
    & Mean (ms) & Std. dev. ($\mu$s) & Mean (ms) & Std. dev. ($\mu$s) \\
    \midrule
    Simple & 3.41 & 11.1 & 3.51 & 264 \\
    Complicated & 369 & 85.0 & 706 & 598 \\
    \bottomrule
\end{tabular} \end{center}

It is interesting to note that while there is little difference in the ``simple'' benchmark, the no-modulus-function method is almost twice as fast in the ``complicated'' benchmark. This is likely because the modulus function variant involves two function calls instead of one. Over many arithmetic operations, these extra calls can cause a significant slowdown.

\appendix
\chapter{Haskell code}

\newmintedfile[haskellfile]{haskell}{linenos,breaklines,mathescape}
\section{Cauchy.hs}
\haskellfile{../Cauchy.hs}
\section{CalkinWilf.hs}
\haskellfile{../CalkinWilf.hs}
\section{CauchyBenchmarks.hs}
\haskellfile{../CauchyBenchmarks.hs}

\bibliographystyle{abbrvnat}
\bibliography{sources}

\nocite{haskell2010}  % Show this reference even though we haven't used it thus far

\end{document}
